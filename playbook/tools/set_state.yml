---
# Input validation tasks
- name: Validate input variables
  block:
    - name: Check required variables
      fail:
        msg: "Missing required variable: {{ item }}"
      when: vars[item] is not defined
      with_items:
        - selected_service
        - target_state
        - ansible_limit
    
    - name: Validate target state
      fail:
        msg: "Invalid target state. Must be 'started', 'stopped', 'restarted', or 'change_role'"
      when: target_state not in ['started', 'stopped', 'restarted', 'change_role']
      
    # Set default variables for batching and safety features
    - name: Set default batch variables
      set_fact:
        batch_size: 5  # Default to 5 nodes per batch
        wait_seconds: 60  # Default to wait 60 seconds between batches
        safety_checks: true  # Default to running safety checks
        skip_health_checks: false  # Default to running health checks
      run_once: true
      delegate_to: localhost
    
    # Validate node role if changing roles
    - name: Validate node role selection
      fail:
        msg: "Missing required variable: node_role_type"
      when: target_state == 'change_role' and (node_role_type is not defined)
    
    - name: Validate node role type
      fail:
        msg: "Invalid node role type. Must be one of: master, hot, frozen, ml"
      when: >
        target_state == 'change_role' and 
        node_role_type not in ['master', 'hot', 'frozen', 'ml']
  run_once: true
  delegate_to: localhost

- name: Configure batch parameters for large clusters
  block:
    - name: Count target hosts
      set_fact:
        target_host_count: "{{ ansible_play_hosts | length }}"
      run_once: true
      delegate_to: localhost
    
    - name: Prompt for batch settings if large target group detected
      pause:
        prompt: |
          You are about to make changes to {{ target_host_count }} hosts.
          For large clusters, a rolling update is recommended.
          
          How many nodes would you like to update per batch? [default: 5]
          (Enter a number between 1 and {{ target_host_count }}, or 'all' to update all at once)
      register: batch_size_input
      run_once: true
      delegate_to: localhost
      when: target_host_count > 10 and (target_state == 'restarted' or target_state == 'stopped')
      
    - name: Prompt for wait time between batches
      pause:
        prompt: |
          How many seconds would you like to wait between batches? [default: 60]
          (This allows time for the cluster to stabilize between batches)
      register: wait_time_input
      run_once: true
      delegate_to: localhost
      when: target_host_count > 10 and (target_state == 'restarted' or target_state == 'stopped') and (batch_size_input.user_input | default('') != 'all')
      
    - name: Set final batch settings
      set_fact:
        final_batch_size: >-
          {% if batch_size_input is not defined or batch_size_input.user_input | default('') == '' %}
            5
          {% elif batch_size_input.user_input | lower == 'all' %}
            {{ target_host_count }}
          {% else %}
            {{ batch_size_input.user_input | int }}
          {% endif %}
        final_wait_seconds: >-
          {% if wait_time_input is not defined or wait_time_input.user_input | default('') == '' %}
            60
          {% else %}
            {{ wait_time_input.user_input | int }}
          {% endif %}
        use_batching: >-
          {% if batch_size_input is not defined %}
            {{ target_host_count > 10 }}
          {% elif batch_size_input.user_input | default('') == 'all' %}
            false
          {% else %}
            true
          {% endif %}
      run_once: true
      delegate_to: localhost
      when: target_host_count > 1
      
    - name: Display batch configuration
      debug:
        msg: |
          Batch configuration:
          - Total hosts: {{ target_host_count }}
          - Hosts per batch: {{ final_batch_size if use_batching else 'All at once' }}
          - Wait between batches: {{ final_wait_seconds if use_batching else 'N/A' }} seconds
      run_once: true
      delegate_to: localhost
      when: target_host_count > 1

- name: Perform Elasticsearch health checks
  block:
    - name: Check if this is an Elasticsearch operation
      set_fact:
        is_elasticsearch_operation: "{{ selected_service == 'elasticsearch' }}"
      run_once: true
      delegate_to: localhost
      
    - name: Get elastic password for API calls
      shell: "cat {{ lookup('env', 'HOME') }}/.elasticsearch/elastic_password.txt 2>/dev/null || echo ''"
      register: elastic_password_lookup
      run_once: true
      delegate_to: localhost
      when: is_elasticsearch_operation
      no_log: true
      
    - name: Set elastic password
      set_fact:
        elastic_password: "{{ elastic_password_lookup.stdout | trim }}"
      run_once: true
      delegate_to: localhost
      when: is_elasticsearch_operation and elastic_password_lookup.stdout | trim != ''
      no_log: true
      
    - name: Check for hot nodes
      set_fact:
        has_hot_nodes: "{{ groups['hot_nodes'] is defined and groups['hot_nodes'] | length > 0 }}"
      run_once: true
      delegate_to: localhost
      when: is_elasticsearch_operation
      
    - name: Check cluster health before changes
      uri:
        url: "https://{{ hostvars[groups['hot_nodes'][0]]['ansible_host'] }}:9200/_cluster/health"
        method: GET
        user: elastic
        password: "{{ elastic_password }}"
        force_basic_auth: yes
        validate_certs: no
        return_content: yes
      register: initial_health
      run_once: true
      delegate_to: localhost
      when: is_elasticsearch_operation and has_hot_nodes and elastic_password is defined
      ignore_errors: yes
      no_log: true
      
    - name: Display initial cluster health
      debug:
        msg: |
          Initial Elasticsearch cluster health:
          - Status: {{ initial_health.json.status | default('unknown') }}
          - Nodes: {{ initial_health.json.number_of_nodes | default('unknown') }}
          - Data nodes: {{ initial_health.json.number_of_data_nodes | default('unknown') }}
          - Active shards: {{ initial_health.json.active_shards | default('unknown') }}
          - Relocating shards: {{ initial_health.json.relocating_shards | default('unknown') }}
          - Initializing shards: {{ initial_health.json.initializing_shards | default('unknown') }}
          - Unassigned shards: {{ initial_health.json.unassigned_shards | default('unknown') }}
      run_once: true
      delegate_to: localhost
      when: is_elasticsearch_operation and has_hot_nodes and initial_health is defined and initial_health.status == 200
      
    - name: Confirm if changes should proceed on the cluster
      pause:
        prompt: |
          About to perform {{ target_state }} operation on {{ target_host_count }} Elasticsearch node(s).
          
          {% if initial_health is defined and initial_health.status == 200 %}
          Current cluster health is {{ initial_health.json.status }}.
          {% else %}
          Unable to determine current cluster health.
          {% endif %}
          
          Do you want to proceed? (yes/no)
      register: es_proceed_confirm
      run_once: true
      delegate_to: localhost
      when: is_elasticsearch_operation and target_host_count > 0
      
    - name: Exit if not confirmed
      meta: end_play
      when: is_elasticsearch_operation and es_proceed_confirm is defined and es_proceed_confirm.user_input | lower != 'yes'
      run_once: true
      delegate_to: localhost

- name: Main utility tasks
  block:
    # Node role management
    - name: Change node role
      block:
        - name: Read current elasticsearch.yml
          shell: "cat /etc/elasticsearch/elasticsearch.yml"
          register: current_es_yaml
          changed_when: false
          failed_when: false
          become: yes
          when: target_state == 'change_role'
          
        - name: Extract current node.roles
          set_fact:
            current_node_roles: "{{ (current_es_yaml.stdout | regex_findall('node\\.roles\\s*:\\s*\\[([^\\]]+)\\]') | first) | default('') }}"
          when: target_state == 'change_role' and current_es_yaml.rc == 0
          
        - name: Set initial node role info
          set_fact:
            local_host_info:
              initial_state: >-
                {%- if current_es_yaml.rc != 0 -%}
                not installed
                {%- elif current_node_roles is defined -%}
                {{ current_node_roles }}
                {%- else -%}
                unknown
                {%- endif -%}
          when: target_state == 'change_role'
          
        - name: Set new node roles based on selected type
          set_fact:
            new_node_roles: "{{ node_roles_map[node_role_type] }}"
          vars:
            node_roles_map:
              master: "[ master ]"
              hot: "[ data_hot, data_content, ingest, ml, transform, remote_cluster_client ]"
              frozen: "[ data_frozen, remote_cluster_client ]"
              ml: "[ ml, remote_cluster_client ]"
          when: target_state == 'change_role'
          
        - name: Update elasticsearch.yml with new node roles
          replace:
            path: /etc/elasticsearch/elasticsearch.yml
            regexp: 'node\.roles\s*:\s*\[[^\]]+\]'
            replace: 'node.roles: {{ new_node_roles }}'
          become: yes
          register: role_change_result
          when: target_state == 'change_role' and local_host_info.initial_state != 'not installed'
          
        - name: Set role change info
          set_fact:
            local_host_info: "{{ local_host_info | combine({
              'action_taken': 'change_role to ' + node_role_type,
              'new_roles': new_node_roles,
              'success': role_change_result is changed,
              'restart_required': true
            }) }}"
          when: target_state == 'change_role' and local_host_info.initial_state != 'not installed'
          
        - name: Prompt to restart elasticsearch after role change
          pause:
            prompt: |
              Node role has been changed to {{ node_role_type }} ({{ new_node_roles }}).
              Elasticsearch must be restarted for changes to take effect.
              
              Do you want to restart Elasticsearch now? (yes/no):
          register: restart_prompt
          delegate_to: localhost
          run_once: true
          when: target_state == 'change_role' and role_change_result is changed
          
        - name: Restart elasticsearch if confirmed
          systemd:
            name: elasticsearch
            state: restarted
          become: yes
          register: restart_result
          when: >
            target_state == 'change_role' and 
            role_change_result is changed and 
            restart_prompt.user_input | lower == 'yes'
          
        - name: Update host info with restart status
          set_fact:
            local_host_info: "{{ local_host_info | combine({
              'restart_performed': restart_prompt.user_input | lower == 'yes',
              'restart_success': restart_result is success
            }) }}"
          when: >
            target_state == 'change_role' and 
            role_change_result is changed and 
            restart_prompt.user_input | lower == 'yes'
      when: target_state == 'change_role'

    # Service state management
    - name: Service state operations
      block:
        - name: Get initial service status
          shell: "systemctl status {{ selected_service }}"
          register: initial_status
          changed_when: false
          failed_when: false
          become: yes

        - name: Set initial state info
          set_fact:
            local_host_info:
              initial_state: >-
                {%- if initial_status.rc != 0 and ('could not be found' in initial_status.stderr or 'no such service' in initial_status.stderr) -%}
                not installed
                {%- elif 'not-found' in initial_status.stdout or 'Unit ' + selected_service + '.service not found' in initial_status.stdout -%}
                not installed
                {%- elif initial_status.rc == 0 and 'active (running)' in initial_status.stdout -%}
                active
                {%- elif initial_status.rc == 0 and 'inactive (dead)' in initial_status.stdout -%}
                inactive
                {%- else -%}
                failed
                {%- endif -%}
              original_hostname: "{{ inventory_hostname }}"
              node_type: >-
                {%- if selected_service == 'elasticsearch' and inventory_hostname in groups['master_nodes'] | default([]) -%}
                master
                {%- elif selected_service == 'elasticsearch' and inventory_hostname in groups['hot_nodes'] | default([]) -%}
                hot
                {%- elif selected_service == 'elasticsearch' and inventory_hostname in groups['frozen_nodes'] | default([]) -%}
                frozen
                {%- elif selected_service == 'elasticsearch' and inventory_hostname in groups['ml_nodes'] | default([]) -%}
                ml
                {%- else -%}
                unknown
                {%- endif -%}
          when: target_state != 'change_role'
          
        # Create host groups for batching
        - name: Share host info for batch grouping
          set_fact:
            all_hosts_info: "{{ all_hosts_info | default({}) | combine({inventory_hostname: local_host_info}) }}"
          run_once: true
          delegate_to: localhost
          when: target_state != 'change_role' and target_host_count > 1
          
        - name: Group hosts by type for ordered operations
          set_fact:
            host_groups:
              unknown_nodes: "{{ ansible_play_hosts | map('extract', hostvars) | selectattr('local_host_info.node_type', 'equalto', 'unknown') | map(attribute='local_host_info.original_hostname') | list }}"
              data_nodes: "{{ ansible_play_hosts | map('extract', hostvars) | selectattr('local_host_info.node_type', 'equalto', 'hot') | map(attribute='local_host_info.original_hostname') | list 
                          + ansible_play_hosts | map('extract', hostvars) | selectattr('local_host_info.node_type', 'equalto', 'frozen') | map(attribute='local_host_info.original_hostname') | list }}"
              ml_nodes: "{{ ansible_play_hosts | map('extract', hostvars) | selectattr('local_host_info.node_type', 'equalto', 'ml') | map(attribute='local_host_info.original_hostname') | list }}"
              master_nodes: "{{ ansible_play_hosts | map('extract', hostvars) | selectattr('local_host_info.node_type', 'equalto', 'master') | map(attribute='local_host_info.original_hostname') | list }}"
          run_once: true
          delegate_to: localhost
          when: target_state != 'change_role' and target_host_count > 1 and selected_service == 'elasticsearch'
          
        - name: Create batches for each node type
          set_fact:
            batched_hosts: "{{ batched_hosts | default({}) | combine({
              'unknown_nodes': unknown_node_batches,
              'data_nodes': data_node_batches,
              'ml_nodes': ml_node_batches,
              'master_nodes': master_node_batches
            }) }}"
          vars:
            unknown_node_batches: "{{ host_groups.unknown_nodes | batch(final_batch_size | int) | list }}"
            data_node_batches: "{{ host_groups.data_nodes | batch(final_batch_size | int) | list }}"
            ml_node_batches: "{{ host_groups.ml_nodes | batch(final_batch_size | int) | list }}"
            master_node_batches: "{{ host_groups.master_nodes | batch(final_batch_size | int) | list }}"
          run_once: true
          delegate_to: localhost
          when: target_state != 'change_role' and use_batching | bool and target_host_count > 1 and selected_service == 'elasticsearch'
          
        - name: Display batch plan
          debug:
            msg: |
              Rolling update plan for Elasticsearch cluster:
              
              1. Unknown nodes: {{ host_groups.unknown_nodes | length }} nodes in {{ batched_hosts.unknown_nodes | length }} batches
              2. Data nodes: {{ host_groups.data_nodes | length }} nodes in {{ batched_hosts.data_nodes | length }} batches
              3. ML nodes: {{ host_groups.ml_nodes | length }} nodes in {{ batched_hosts.ml_nodes | length }} batches
              4. Master nodes: {{ host_groups.master_nodes | length }} nodes in {{ batched_hosts.master_nodes | length }} batches
              
              Total: {{ target_host_count }} nodes
              Wait time between batches: {{ final_wait_seconds }} seconds
          run_once: true
          delegate_to: localhost
          when: target_state != 'change_role' and use_batching | bool and target_host_count > 1 and selected_service == 'elasticsearch' and batched_hosts is defined
      
        # Execution for non-batched operations (non-Elasticsearch or small clusters)
        - name: Execute standard service state change (no batching)
          block:
            - name: Manage service state (no batching)
              systemd:
                name: "{{ selected_service }}"
                state: "{{ target_state }}"
              failed_when: false
              become: yes
              register: state_change_result
              when: local_host_info.initial_state != 'not installed'
              
            - name: Wait for service startup
              wait_for:
                timeout: 10
              when: 
                - target_state not in ['stopped', 'change_role']
                - local_host_info.initial_state != 'not installed'
          when: >
            target_state != 'change_role' and 
            (not use_batching | bool or selected_service != 'elasticsearch')
            
        # Execution for batched operations (Elasticsearch with many nodes)
        - name: Execute batched service state changes
          block:
            # Unknown nodes - Process in batches
            - name: Process unknown node batches
              include_tasks: "{{ playbook_dir }}/tools/process_node_batch.yml"
              vars:
                current_batch: "{{ item }}"
                node_type: "unknown"
                batch_number: "{{ loop.index }}"
                total_batches: "{{ batched_hosts.unknown_nodes | length }}"
              with_items: "{{ batched_hosts.unknown_nodes }}"
              run_once: true
              delegate_to: localhost
              when: batched_hosts.unknown_nodes | length > 0
              
            # Data nodes - Process in batches
            - name: Process data node batches
              include_tasks: "{{ playbook_dir }}/tools/process_node_batch.yml"
              vars:
                current_batch: "{{ item }}"
                node_type: "data"
                batch_number: "{{ loop.index }}"
                total_batches: "{{ batched_hosts.data_nodes | length }}"
              with_items: "{{ batched_hosts.data_nodes }}"
              run_once: true
              delegate_to: localhost
              when: batched_hosts.data_nodes | length > 0
            
            # ML nodes - Process in batches  
            - name: Process ML node batches
              include_tasks: "{{ playbook_dir }}/tools/process_node_batch.yml"
              vars:
                current_batch: "{{ item }}"
                node_type: "ml"
                batch_number: "{{ loop.index }}"
                total_batches: "{{ batched_hosts.ml_nodes | length }}"
              with_items: "{{ batched_hosts.ml_nodes }}"
              run_once: true
              delegate_to: localhost
              when: batched_hosts.ml_nodes | length > 0
              
            # Master nodes - Process in batches (last)
            - name: Process master node batches
              include_tasks: "{{ playbook_dir }}/tools/process_node_batch.yml"
              vars:
                current_batch: "{{ item }}"
                node_type: "master"
                batch_number: "{{ loop.index }}"
                total_batches: "{{ batched_hosts.master_nodes | length }}"
              with_items: "{{ batched_hosts.master_nodes }}"
              run_once: true
              delegate_to: localhost
              when: batched_hosts.master_nodes | length > 0
          when: >
            target_state != 'change_role' and 
            use_batching | bool and 
            target_host_count > 1 and 
            selected_service == 'elasticsearch' and 
            batched_hosts is defined

        - name: Get final service status
          shell: "systemctl status {{ selected_service }}"
          register: final_status
          changed_when: false
          failed_when: false
          become: yes
          when: target_state != 'change_role'

        - name: Get journalctl logs if failed
          shell: "journalctl -u {{ selected_service }}.service -n 50 --no-pager | tail -n 5"
          register: journal_logs_result
          when: target_state != 'change_role' and "'failed' in final_status.stdout or final_status.rc != 0"
          changed_when: false
          become: yes
          ignore_errors: yes

        - name: Update host info with final state
          set_fact:
            local_host_info: "{{ local_host_info | combine({
              'final_state': final_state,
              'action_taken': target_state,
              'success': success_state,
              'journal_logs': journal_logs_result.stdout | default('')
            }) }}"
          vars:
            final_state: >-
              {%- if final_status.rc != 0 and ('could not be found' in final_status.stderr or 'no such service' in final_status.stderr) -%}
              not installed
              {%- elif 'not-found' in final_status.stdout or 'Unit ' + selected_service + '.service not found' in final_status.stdout -%}
              not installed
              {%- elif final_status.rc == 0 and 'active (running)' in final_status.stdout -%}
              active
              {%- elif final_status.rc == 0 and 'inactive (dead)' in final_status.stdout -%}
              inactive
              {%- else -%}
              failed
              {%- endif -%}
            success_state: >-
              {%- if local_host_info.initial_state == 'not installed' -%}
              false
              {%- elif target_state == 'started' and final_state == 'active' -%}
              true
              {%- elif target_state == 'stopped' and final_state == 'inactive' -%}
              true
              {%- elif target_state == 'restarted' and final_state == 'active' -%}
              true
              {%- else -%}
              false
              {%- endif -%}
          when: target_state != 'change_role'
      when: target_state != 'change_role'

    # Report generation
    # Perform final health check for Elasticsearch
    - name: Final Elasticsearch health verification
      block:
        - name: Check final cluster health
          uri:
            url: "https://{{ hostvars[groups['hot_nodes'][0]]['ansible_host'] }}:9200/_cluster/health"
            method: GET
            user: elastic
            password: "{{ elastic_password }}"
            force_basic_auth: yes
            validate_certs: no
            return_content: yes
          register: final_health
          run_once: true
          delegate_to: localhost
          when: is_elasticsearch_operation and has_hot_nodes and elastic_password is defined
          ignore_errors: yes
          no_log: true
          
        - name: Wait longer for cluster stabilization
          pause:
            seconds: 30
          run_once: true
          delegate_to: localhost
          when: is_elasticsearch_operation and final_health is defined and final_health.status == 200 and final_health.json.status != 'green'
          
        - name: Check final cluster health again
          uri:
            url: "https://{{ hostvars[groups['hot_nodes'][0]]['ansible_host'] }}:9200/_cluster/health"
            method: GET
            user: elastic
            password: "{{ elastic_password }}"
            force_basic_auth: yes
            validate_certs: no
            return_content: yes
          register: final_health_retry
          run_once: true
          delegate_to: localhost
          when: is_elasticsearch_operation and has_hot_nodes and elastic_password is defined and final_health is defined and final_health.json.status != 'green'
          ignore_errors: yes
          no_log: true
          
        - name: Display final cluster health
          debug:
            msg: |
              FINAL ELASTICSEARCH CLUSTER HEALTH
              ================================
              
              Status: {{ (final_health_retry.json.status | default(final_health.json.status)) | default('unknown') | upper }}
              Nodes: {{ (final_health_retry.json.number_of_nodes | default(final_health.json.number_of_nodes)) | default('unknown') }}
              Data Nodes: {{ (final_health_retry.json.number_of_data_nodes | default(final_health.json.number_of_data_nodes)) | default('unknown') }}
              Active Shards: {{ (final_health_retry.json.active_shards | default(final_health.json.active_shards)) | default('unknown') }}
              
              Shard Status:
              - Initializing: {{ (final_health_retry.json.initializing_shards | default(final_health.json.initializing_shards)) | default('unknown') }}
              - Relocating: {{ (final_health_retry.json.relocating_shards | default(final_health.json.relocating_shards)) | default('unknown') }}
              - Unassigned: {{ (final_health_retry.json.unassigned_shards | default(final_health.json.unassigned_shards)) | default('unknown') }}
              
              {% if (final_health_retry.json.status | default(final_health.json.status)) == 'yellow' %}
              NOTE: Cluster status is YELLOW. This usually indicates that some replica shards
              are not allocated. The cluster is operational but does not have full redundancy.
              {% elif (final_health_retry.json.status | default(final_health.json.status)) == 'red' %}
              WARNING: Cluster status is RED. Some primary shards are not allocated.
              This indicates data may be unavailable. Immediate investigation is required.
              {% elif (final_health_retry.json.status | default(final_health.json.status)) == 'green' %}
              SUCCESS: Cluster status is GREEN. All primary and replica shards are allocated.
              The cluster is fully operational with complete redundancy.
              {% endif %}
              
              Operation completed at: {{ ansible_date_time.iso8601 }}
          run_once: true
          delegate_to: localhost
          when: is_elasticsearch_operation and final_health is defined and final_health.status == 200
          
      when: >
        target_state != 'change_role' and 
        selected_service == 'elasticsearch' and 
        is_elasticsearch_operation and 
        has_hot_nodes and 
        elastic_password is defined
          
    - name: Generate reports
      block:
        - name: Ensure report directory exists on localhost
          file:
            path: "/tmp/service_reports"
            state: directory
          run_once: true
          delegate_to: localhost

        - name: Share host info with localhost
          set_fact:
            shared_host_info: "{{ shared_host_info | default({}) | combine({inventory_hostname: local_host_info}) }}"
          delegate_to: localhost

        - name: Create status report for service state change
          copy:
            content: |
              Service State Change Report for {{ selected_service }}
              ==============================================
              Host: {{ inventory_hostname }}
              ----------------------------------------------
              Requested Change: {{ local_host_info.action_taken }}
              Initial State: {{ local_host_info.initial_state }}
              Final State: {{ local_host_info.final_state | default('N/A') }}
              Success: {{ local_host_info.success }}
              {% if local_host_info.initial_state == 'not installed' %}
              Details: Service is not installed on this host (Unit {{ selected_service }}.service not found)
              {% endif %}
              {% if state_change_result is defined and state_change_result.msg is defined %}
              Operation Message: {{ state_change_result.msg }}
              {% endif %}
              {% if local_host_info.journal_logs is defined and local_host_info.journal_logs != '' %}
              
              Recent Logs:
              {{ local_host_info.journal_logs }}
              {% endif %}
            dest: "/tmp/service_reports/status_{{ inventory_hostname }}.txt"
          delegate_to: localhost
          when: target_state != 'change_role'
          
        - name: Create status report for node role change
          copy:
            content: |
              Node Role Change Report for {{ selected_service }}
              ==============================================
              Host: {{ inventory_hostname }}
              ----------------------------------------------
              Requested Change: Change node role to {{ node_role_type }}
              Previous Roles: {{ local_host_info.initial_state }}
              New Roles: {{ local_host_info.new_roles | default('N/A') }}
              Change Successful: {{ local_host_info.success }}
              Restart Required: {{ local_host_info.restart_required | default(false) }}
              {% if local_host_info.restart_performed is defined %}
              Restart Performed: {{ local_host_info.restart_performed }}
              Restart Successful: {{ local_host_info.restart_success | default(false) }}
              {% endif %}
              {% if local_host_info.initial_state == 'not installed' %}
              Details: Elasticsearch is not installed on this host
              {% endif %}
            dest: "/tmp/service_reports/status_{{ inventory_hostname }}.txt"
          delegate_to: localhost
          when: target_state == 'change_role'

        - name: Collect all reports
          shell: "cat /tmp/service_reports/status_*.txt"
          register: combined_status
          run_once: true
          delegate_to: localhost

        - name: Show status report
          debug:
            msg: "{{ combined_status.stdout_lines }}"
          run_once: true
          delegate_to: localhost
      rescue:
        - name: Handle report generation failure
          debug:
            msg: "Failed to generate status reports. Check disk space and permissions."
          run_once: true
          delegate_to: localhost
      always:
        - name: Cleanup report files
          file:
            path: "/tmp/service_reports"
            state: absent
          run_once: true
          delegate_to: localhost